#!/usr/bin/env python3
"""
üöÄ Orchestrateur du bot (filters + WebSocket prix) - VERSION OPTIMIS√âE

Script pour filtrer les contrats perp√©tuels par funding ET suivre leurs prix en temps r√©el.

OPTIMISATIONS PERFORMANCE APPLIQU√âES:
- fetch_spread_data(): batch_size augment√© de 50 √† 200 (limite max API Bybit)
- fetch_spread_data(): suppression des d√©lais time.sleep(0.1) entre batches
- fetch_spread_data(): parall√©lisation avec ThreadPoolExecutor (max 4 workers)
- fetch_funding_map(): limite maintenue √† 1000 (maximum support√© par l'API)
- filter_by_volatility(): parall√©lisation async/await avec aiohttp et asyncio.gather()
- Gestion d'erreur robuste pour les requ√™tes parall√®les
- R√©duction estim√©e du temps de r√©cup√©ration: 60-70% (spreads) + 80-90% (volatilit√©)

Usage:
    python src/bot.py
"""

import os
import sys
import json
import time
import signal
import threading
import yaml
import httpx
import websocket
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from http_utils import get_rate_limiter
try:
    from .config import get_settings
except ImportError:
    from config import get_settings
from logging_setup import setup_logging
from bybit_client import BybitClient, BybitPublicClient
from instruments import get_perp_symbols, category_of_symbol
from price_store import update, get_snapshot, purge_expired
from volatility import get_volatility_cache_key, is_cache_valid, compute_volatility_batch_async
from errors import NoSymbolsError, FundingUnavailableError
from metrics import record_filter_result, record_ws_connection, record_ws_error
from metrics_monitor import start_metrics_monitoring


class PublicWSConnection:
    """Connexion WebSocket publique isol√©e (√©vite l'√©tat partag√© entre linear/inverse)."""

    def __init__(self, category: str, symbols: list[str], testnet: bool, logger, on_ticker_callback):
        self.category = category
        self.symbols = symbols
        self.testnet = testnet
        self.logger = logger
        self.on_ticker_callback = on_ticker_callback  # callable(ticker_data: dict)
        self.ws = None
        self.running = False
        
        # Configuration de reconnexion avec backoff
        self.reconnect_delays = [1, 2, 5, 10, 30]  # secondes
        self.current_delay_index = 0

    def _build_url(self) -> str:
        if self.category == "linear":
            return "wss://stream-testnet.bybit.com/v5/public/linear" if self.testnet else "wss://stream.bybit.com/v5/public/linear"
        else:
            return "wss://stream-testnet.bybit.com/v5/public/inverse" if self.testnet else "wss://stream.bybit.com/v5/public/inverse"

    def _on_open(self, ws):
        self.logger.info(f"üåê WS ouverte ({self.category})")
        
        # Enregistrer la connexion WebSocket
        record_ws_connection(connected=True)
        
        # R√©initialiser l'index de d√©lai de reconnexion apr√®s une connexion r√©ussie
        self.current_delay_index = 0
        
        # S'abonner aux tickers pour tous les symboles
        if self.symbols:
            subscribe_message = {
                "op": "subscribe",
                "args": [f"tickers.{symbol}" for symbol in self.symbols]
            }
            try:
                ws.send(json.dumps(subscribe_message))
                self.logger.info(f"üß≠ Souscription tickers ‚Üí {len(self.symbols)} symboles ({self.category})")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erreur souscription {self.category}: {e}")
        else:
            self.logger.warning(f"‚ö†Ô∏è Aucun symbole √† suivre pour {self.category}")

    def _on_message(self, ws, message):
        try:
            data = json.loads(message)
            if data.get("topic", "").startswith("tickers."):
                ticker_data = data.get("data", {})
                if ticker_data:
                    self.on_ticker_callback(ticker_data)
        except json.JSONDecodeError as e:
            self.logger.warning(f"‚ö†Ô∏è Erreur JSON ({self.category}): {e}")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erreur parsing ({self.category}): {e}")

    def _on_error(self, ws, error):
        if self.running:
            self.logger.warning(f"‚ö†Ô∏è WS erreur ({self.category}) : {error}")
            record_ws_error()

    def _on_close(self, ws, close_status_code, close_msg):
        if self.running:
            self.logger.info(f"üîå WS ferm√©e ({self.category}) (code={close_status_code}, reason={close_msg})")

    def run(self):
        """Boucle principale avec reconnexion automatique et backoff progressif."""
        self.running = True
        
        while self.running:
            try:
                try:
                    self.logger.info(f"üîê Connexion √† la WebSocket publique ({self.category})‚Ä¶")
                except Exception:
                    pass
                
                url = self._build_url()
                self.ws = websocket.WebSocketApp(
                    url,
                    on_open=self._on_open,
                    on_message=self._on_message,
                    on_error=self._on_error,
                    on_close=self._on_close
                )
                
                self.ws.run_forever(ping_interval=20, ping_timeout=10)
                
            except Exception as e:
                if self.running:
                    try:
                        self.logger.error(f"Erreur connexion WS publique ({self.category}): {e}")
                    except Exception:
                        pass
            
            # Reconnexion avec backoff progressif
            if self.running:
                delay = self.reconnect_delays[min(self.current_delay_index, len(self.reconnect_delays) - 1)]
                try:
                    self.logger.warning(f"üîÅ WS publique ({self.category}) d√©connect√©e ‚Üí reconnexion dans {delay}s")
                    record_ws_connection(connected=False)  # Enregistrer la reconnexion
                except Exception:
                    pass
                
                # Attendre le d√©lai avec v√©rification p√©riodique de l'arr√™t
                for _ in range(delay):
                    if not self.running:
                        break
                    time.sleep(1)
                
                # Augmenter l'index de d√©lai pour le prochain backoff (jusqu'√† la limite)
                if self.current_delay_index < len(self.reconnect_delays) - 1:
                    self.current_delay_index += 1
            else:
                break

    def close(self):
        self.running = False
        if self.ws:
            try:
                self.ws.close()
            except Exception:
                pass

def validate_config(config: dict) -> None:
    """
    Valide la coh√©rence des param√®tres de configuration.
    
    Args:
        config (dict): Configuration √† valider
        
    Raises:
        ValueError: Si des param√®tres sont incoh√©rents ou invalides
    """
    errors = []
    
    # Validation des bornes de funding
    funding_min = config.get("funding_min")
    funding_max = config.get("funding_max")
    
    if funding_min is not None and funding_max is not None:
        if funding_min > funding_max:
            errors.append(f"funding_min ({funding_min}) ne peut pas √™tre sup√©rieur √† funding_max ({funding_max})")
    
    # Validation des bornes de volatilit√©
    volatility_min = config.get("volatility_min")
    volatility_max = config.get("volatility_max")
    
    if volatility_min is not None and volatility_max is not None:
        if volatility_min > volatility_max:
            errors.append(f"volatility_min ({volatility_min}) ne peut pas √™tre sup√©rieur √† volatility_max ({volatility_max})")
    
    # Validation des valeurs n√©gatives
    if funding_min is not None and funding_min < 0:
        errors.append(f"funding_min ne peut pas √™tre n√©gatif ({funding_min})")
    
    if funding_max is not None and funding_max < 0:
        errors.append(f"funding_max ne peut pas √™tre n√©gatif ({funding_max})")
    
    if volatility_min is not None and volatility_min < 0:
        errors.append(f"volatility_min ne peut pas √™tre n√©gatif ({volatility_min})")
    
    if volatility_max is not None and volatility_max < 0:
        errors.append(f"volatility_max ne peut pas √™tre n√©gatif ({volatility_max})")
    
    # Validation du spread
    spread_max = config.get("spread_max")
    if spread_max is not None:
        if spread_max < 0:
            errors.append(f"spread_max ne peut pas √™tre n√©gatif ({spread_max})")
        if spread_max > 1.0:  # 100% de spread maximum
            errors.append(f"spread_max trop √©lev√© ({spread_max}), maximum recommand√©: 1.0 (100%)")
    
    # Validation des volumes
    volume_min = config.get("volume_min")
    volume_min_millions = config.get("volume_min_millions")
    
    if volume_min is not None and volume_min < 0:
        errors.append(f"volume_min ne peut pas √™tre n√©gatif ({volume_min})")
    
    if volume_min_millions is not None and volume_min_millions < 0:
        errors.append(f"volume_min_millions ne peut pas √™tre n√©gatif ({volume_min_millions})")
    
    # Validation des param√®tres temporels de funding
    ft_min = config.get("funding_time_min_minutes")
    ft_max = config.get("funding_time_max_minutes")
    
    if ft_min is not None:
        if ft_min < 0:
            errors.append(f"funding_time_min_minutes ne peut pas √™tre n√©gatif ({ft_min})")
        if ft_min > 1440:  # 24 heures maximum
            errors.append(f"funding_time_min_minutes trop √©lev√© ({ft_min}), maximum: 1440 (24h)")
    
    if ft_max is not None:
        if ft_max < 0:
            errors.append(f"funding_time_max_minutes ne peut pas √™tre n√©gatif ({ft_max})")
        if ft_max > 1440:  # 24 heures maximum
            errors.append(f"funding_time_max_minutes trop √©lev√© ({ft_max}), maximum: 1440 (24h)")
    
    if ft_min is not None and ft_max is not None:
        if ft_min > ft_max:
            errors.append(f"funding_time_min_minutes ({ft_min}) ne peut pas √™tre sup√©rieur √† funding_time_max_minutes ({ft_max})")
    
    # Validation de la cat√©gorie
    categorie = config.get("categorie")
    if categorie not in ["linear", "inverse", "both"]:
        errors.append(f"categorie invalide ({categorie}), valeurs autoris√©es: linear, inverse, both")
    
    # Validation de la limite
    limite = config.get("limite")
    if limite is not None:
        if limite < 1:
            errors.append(f"limite doit √™tre positive ({limite})")
        if limite > 1000:
            errors.append(f"limite trop √©lev√©e ({limite}), maximum recommand√©: 1000")
    
    # Validation du TTL de volatilit√©
    vol_ttl = config.get("volatility_ttl_sec")
    if vol_ttl is not None:
        if vol_ttl < 10:
            errors.append(f"volatility_ttl_sec trop faible ({vol_ttl}), minimum: 10 secondes")
        if vol_ttl > 3600:
            errors.append(f"volatility_ttl_sec trop √©lev√© ({vol_ttl}), maximum: 3600 secondes (1h)")
    
    # Lever une erreur si des probl√®mes ont √©t√© d√©tect√©s
    if errors:
        error_msg = "Configuration invalide d√©tect√©e:\n" + "\n".join(f"  - {error}" for error in errors)
        raise ValueError(error_msg)


def load_config() -> dict:
    """
    Charge la configuration depuis le fichier YAML ou utilise les valeurs par d√©faut.
    Priorit√© : ENV > fichier > valeurs par d√©faut
    
    Returns:
        dict: Configuration avec categorie, funding_min, funding_max, volume_min_millions, limite
    """
    config_path = "src/parameters.yaml"
    
    # Valeurs par d√©faut
    default_config = {
        "categorie": "linear",
        "funding_min": None,
        "funding_max": None,
        "volume_min": None,
        "volume_min_millions": None,
        "spread_max": None,
        "volatility_min": None,
        "volatility_max": None,
        "limite": 10,
        "volatility_ttl_sec": 120,
        # Nouveaux param√®tres temporels
        "funding_time_min_minutes": None,
        "funding_time_max_minutes": None,
    }
    
    # Charger depuis le fichier si disponible
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            file_config = yaml.safe_load(f)
        if file_config:
            default_config.update(file_config)
    except FileNotFoundError:
        pass  # Utiliser les valeurs par d√©faut
    
    # R√©cup√©rer les variables d'environnement (priorit√© maximale)
    settings = get_settings()
    env_spread_max = settings.get("spread_max")
    env_volume_min_millions = settings.get("volume_min_millions")
    env_volatility_min = settings.get("volatility_min")
    env_volatility_max = settings.get("volatility_max")
    # Alignement ENV suppl√©mentaires (README)
    env_funding_min = settings.get("funding_min")
    env_funding_max = settings.get("funding_max")
    env_category = settings.get("category")
    env_limit = settings.get("limit")
    env_vol_ttl = settings.get("volatility_ttl_sec")
    # Nouveaux ENV temporels
    env_ft_min = settings.get("funding_time_min_minutes")
    env_ft_max = settings.get("funding_time_max_minutes")
    
    # Appliquer les variables d'environnement si pr√©sentes
    if env_spread_max is not None:
        default_config["spread_max"] = env_spread_max
    if env_volume_min_millions is not None:
        default_config["volume_min_millions"] = env_volume_min_millions
    if env_volatility_min is not None:
        default_config["volatility_min"] = env_volatility_min
    if env_volatility_max is not None:
        default_config["volatility_max"] = env_volatility_max
    if env_funding_min is not None:
        default_config["funding_min"] = env_funding_min
    if env_funding_max is not None:
        default_config["funding_max"] = env_funding_max
    if env_category is not None:
        default_config["categorie"] = env_category
    if env_limit is not None:
        default_config["limite"] = env_limit
    if env_vol_ttl is not None:
        default_config["volatility_ttl_sec"] = env_vol_ttl
    # Appliquer les ENV temporels
    if env_ft_min is not None:
        default_config["funding_time_min_minutes"] = env_ft_min
    if env_ft_max is not None:
        default_config["funding_time_max_minutes"] = env_ft_max
    
    # Valider la configuration finale
    validate_config(default_config)
    
    return default_config


def fetch_funding_map(base_url: str, category: str, timeout: int) -> dict[str, float]:
    """
    R√©cup√®re les taux de funding pour une cat√©gorie donn√©e.
    OPTIMIS√â: Utilise la limite maximum de l'API Bybit (1000) et une gestion d'erreur robuste.
    
    Args:
        base_url (str): URL de base de l'API Bybit
        category (str): Cat√©gorie (linear ou inverse)
        timeout (int): Timeout pour les requ√™tes HTTP
        
    Returns:
        dict[str, float]: Dictionnaire {symbol: funding_rate}
        
    Raises:
        RuntimeError: En cas d'erreur HTTP ou API
    """
    funding_map = {}
    cursor = ""
    page_index = 0
    
    rate_limiter = get_rate_limiter()
    while True:
        # Construire l'URL avec pagination
        url = f"{base_url}/v5/market/tickers"
        params = {
            "category": category,
            "limit": 1000  # OPTIMISATION: Limite maximum support√©e par l'API Bybit
        }
        if cursor:
            params["cursor"] = cursor
            
        try:
            page_index += 1
            # Respecter le rate limit avant chaque appel
            rate_limiter.acquire()
            with httpx.Client(timeout=timeout) as client:
                response = client.get(url, params=params)
                
                # V√©rifier le statut HTTP
                if response.status_code >= 400:
                    raise RuntimeError(
                        (
                            f"Erreur HTTP Bybit GET {url} | category={category} limit={params.get('limit')} "
                            f"cursor={params.get('cursor', '-') } timeout={timeout}s page={page_index} "
                            f"collected={len(funding_map)} | status={response.status_code} "
                            f"detail=\"{response.text[:200]}\""
                        )
                    )
                
                data = response.json()
                
                # V√©rifier le retCode
                if data.get("retCode") != 0:
                    ret_code = data.get("retCode")
                    ret_msg = data.get("retMsg", "")
                    raise RuntimeError(
                        (
                            f"Erreur API Bybit GET {url} | category={category} limit={params.get('limit')} "
                            f"cursor={params.get('cursor', '-') } timeout={timeout}s page={page_index} "
                            f"collected={len(funding_map)} | retCode={ret_code} retMsg=\"{ret_msg}\""
                        )
                    )
                
                result = data.get("result", {})
                tickers = result.get("list", [])
                
                # Extraire les funding rates, volumes et temps de funding
                for ticker in tickers:
                    symbol = ticker.get("symbol", "")
                    funding_rate = ticker.get("fundingRate")
                    volume_24h = ticker.get("volume24h")
                    next_funding_time = ticker.get("nextFundingTime")
                    
                    if symbol and funding_rate is not None:
                        try:
                            funding_map[symbol] = {
                                "funding": float(funding_rate),
                                "volume": float(volume_24h) if volume_24h is not None else 0.0,
                                "next_funding_time": next_funding_time
                            }
                        except (ValueError, TypeError):
                            # Ignorer si les donn√©es ne sont pas convertibles en float
                            pass
                
                # V√©rifier s'il y a une page suivante
                next_page_cursor = result.get("nextPageCursor")
                if not next_page_cursor:
                    break
                cursor = next_page_cursor
                
        except httpx.RequestError as e:
            raise RuntimeError(
                (
                    f"Erreur r√©seau Bybit GET {url} | category={category} limit={params.get('limit')} "
                    f"cursor={params.get('cursor', '-') } timeout={timeout}s page={page_index} "
                    f"collected={len(funding_map)} | error={e}"
                )
            )
        except Exception as e:
            if "Erreur" in str(e):
                raise
            else:
                raise RuntimeError(
                    (
                        f"Erreur inconnue Bybit GET {url} | category={category} limit={params.get('limit')} "
                        f"cursor={params.get('cursor', '-') } timeout={timeout}s page={page_index} "
                        f"collected={len(funding_map)} | error={e}"
                    )
                )
    
    return funding_map


def calculate_funding_time_remaining(next_funding_time: str | int | float | None) -> str:
    """Retourne "Xh Ym Zs" √† partir d'un timestamp Bybit (ms) ou ISO. Pas de parsing manuel."""
    if not next_funding_time:
        return "-"
    try:
        import datetime
        funding_dt: datetime.datetime | None = None
        if isinstance(next_funding_time, (int, float)):
            funding_dt = datetime.datetime.fromtimestamp(float(next_funding_time) / 1000, tz=datetime.timezone.utc)
        elif isinstance(next_funding_time, str):
            if next_funding_time.isdigit():
                funding_dt = datetime.datetime.fromtimestamp(float(next_funding_time) / 1000, tz=datetime.timezone.utc)
            else:
                funding_dt = datetime.datetime.fromisoformat(next_funding_time.replace('Z', '+00:00'))
                if funding_dt.tzinfo is None:
                    funding_dt = funding_dt.replace(tzinfo=datetime.timezone.utc)
        if funding_dt is None:
            return "-"
        now = datetime.datetime.now(datetime.timezone.utc)
        delta = (funding_dt - now).total_seconds()
        if delta <= 0:
            return "-"
        total_seconds = int(delta)
        hours = total_seconds // 3600
        minutes = (total_seconds % 3600) // 60
        seconds = total_seconds % 60
        if hours > 0:
            return f"{hours}h {minutes}m {seconds}s"
        if minutes > 0:
            return f"{minutes}m {seconds}s"
        return f"{seconds}s"
    except Exception:
        return "-"


def fetch_spread_data(base_url: str, symbols: list[str], timeout: int, category: str = "linear") -> dict[str, float]:
    """
    R√©cup√®re les spreads via /v5/market/tickers pagin√© (sans symbol=), puis filtre localement.
    
    Args:
        base_url (str): URL de base de l'API Bybit
        symbols (list[str]): Liste cible des symboles √† retourner
        timeout (int): Timeout HTTP
        category (str): "linear" ou "inverse"
    Returns:
        dict[str, float]: map {symbol: spread_pct}
    """
    wanted = set(symbols)
    found: dict[str, float] = {}
    url = f"{base_url}/v5/market/tickers"
    params = {"category": category, "limit": 1000}
    cursor = ""
    page_index = 0
    rate_limiter = get_rate_limiter()
    while True:
        page_index += 1
        if cursor:
            params["cursor"] = cursor
        else:
            params.pop("cursor", None)
        try:
            rate_limiter.acquire()
            with httpx.Client(timeout=timeout) as client:
                resp = client.get(url, params=params)
                if resp.status_code >= 400:
                    raise RuntimeError(
                        (
                            f"Erreur HTTP Bybit GET {url} | category={category} page={page_index} "
                            f"limit={params.get('limit')} cursor={params.get('cursor','-')} status={resp.status_code} "
                            f"detail=\"{resp.text[:200]}\""
                        )
                    )
                data = resp.json()
                if data.get("retCode") != 0:
                    raise RuntimeError(
                        (
                            f"Erreur API Bybit GET {url} | category={category} page={page_index} "
                            f"retCode={data.get('retCode')} retMsg=\"{data.get('retMsg','')}\""
                        )
                    )
                result = data.get("result", {})
                tickers = result.get("list", [])
                for t in tickers:
                    sym = t.get("symbol")
                    if sym in wanted:
                        bid1 = t.get("bid1Price")
                        ask1 = t.get("ask1Price")
                        try:
                            if bid1 is not None and ask1 is not None:
                                b = float(bid1)
                                a = float(ask1)
                                if b > 0 and a > 0:
                                    mid = (a + b) / 2
                                    if mid > 0:
                                        found[sym] = (a - b) / mid
                        except Exception:
                            pass
                # Fin pagination
                next_cursor = result.get("nextPageCursor")
                # Arr√™t anticip√© si on a tout trouv√©
                if len(found) >= len(wanted):
                    break
                if not next_cursor:
                    break
                cursor = next_cursor
        except Exception as e:
            # On ne stoppe pas le flux complet: on tente une r√©cup√©ration unitaire pour les manquants
            try:
                from logging_setup import setup_logging
                setup_logging().warning(
                    f"‚ö†Ô∏è Spread pagin√© erreur page={page_index} category={category} error={e}"
                )
            except Exception:
                pass
            break
    # Fallback unitaire pour les symboles manquants
    missing = [s for s in symbols if s not in found]
    for s in missing:
        try:
            val = _fetch_single_spread(base_url, s, timeout, category)
            if val is not None:
                found[s] = val
        except Exception:
            pass
    return found


def _process_batch_spread(base_url: str, symbol_batch: list[str], timeout: int, category: str, batch_idx: int) -> dict[str, float]:
    """
    Traite un batch de symboles pour r√©cup√©rer les spreads.
    Fonction helper pour la parall√©lisation.
    
    Args:
        base_url (str): URL de base de l'API Bybit
        symbol_batch (list[str]): Batch de symboles √† traiter
        timeout (int): Timeout pour les requ√™tes HTTP
        category (str): Cat√©gorie des symboles
        batch_idx (int): Index du batch (pour le logging)
        
    Returns:
        dict[str, float]: Dictionnaire {symbol: spread_pct} pour ce batch
    """
    batch_spread_data = {}
    
    try:
        # Construire l'URL avec les symboles du batch
        url = f"{base_url}/v5/market/tickers"
        params = {
            "category": category,
            "symbol": ",".join(symbol_batch)
        }
        
        # Respecter le rate limiter global
        rate_limiter = get_rate_limiter()
        rate_limiter.acquire()
        with httpx.Client(timeout=timeout) as client:
            response = client.get(url, params=params)
            
            # V√©rifier le statut HTTP
            if response.status_code >= 400:
                sample = ",".join(symbol_batch[:5])
                raise RuntimeError(
                    (
                        f"Erreur HTTP Bybit GET {url} | category={category} batch_idx={batch_idx} "
                        f"batch_size={len(symbol_batch)} sample=[{sample}] timeout={timeout}s "
                        f"status={response.status_code} detail=\"{response.text[:200]}\""
                    )
                )
            
            data = response.json()
            
            # V√©rifier le retCode
            if data.get("retCode") != 0:
                ret_code = data.get("retCode")
                ret_msg = data.get("retMsg", "")
                
                # Si erreur de symbole invalide, essayer un par un
                if ret_code == 10001 and "symbol invalid" in ret_msg.lower():
                    # R√©cup√©rer les spreads un par un pour ce batch
                    for symbol in symbol_batch:
                        try:
                            single_spread = _fetch_single_spread(base_url, symbol, timeout, category)
                            if single_spread:
                                batch_spread_data[symbol] = single_spread
                        except Exception:
                            # Ignorer les symboles qui √©chouent
                            pass
                    return batch_spread_data
                else:
                    sample = ",".join(symbol_batch[:5])
                    raise RuntimeError(
                        (
                            f"Erreur API Bybit GET {url} | category={category} batch_idx={batch_idx} "
                            f"batch_size={len(symbol_batch)} sample=[{sample}] timeout={timeout}s "
                            f"retCode={ret_code} retMsg=\"{ret_msg}\""
                        )
                    )
            
            result = data.get("result", {})
            tickers = result.get("list", [])
            
            # Extraire les donn√©es de spread
            for ticker in tickers:
                symbol = ticker.get("symbol", "")
                bid1_price = ticker.get("bid1Price")
                ask1_price = ticker.get("ask1Price")
                
                if symbol and bid1_price is not None and ask1_price is not None:
                    try:
                        bid1 = float(bid1_price)
                        ask1 = float(ask1_price)
                        
                        # Calculer le spread en pourcentage
                        # spread_pct = (ask1 - bid1) / ((ask1 + bid1)/2)
                        if bid1 > 0 and ask1 > 0:
                            spread_pct = (ask1 - bid1) / ((ask1 + bid1) / 2)
                            batch_spread_data[symbol] = spread_pct
                    except (ValueError, TypeError, ZeroDivisionError):
                        # Ignorer si les donn√©es ne sont pas convertibles ou si division par z√©ro
                        pass
                
    except httpx.RequestError as e:
        sample = ",".join(symbol_batch[:5])
        raise RuntimeError(
            (
                f"Erreur r√©seau Bybit GET {url} | category={category} batch_idx={batch_idx} "
                f"batch_size={len(symbol_batch)} sample=[{sample}] timeout={timeout}s error={e}"
            )
        )
    except Exception as e:
        if "Erreur" in str(e):
            raise
        else:
            sample = ",".join(symbol_batch[:5])
            raise RuntimeError(
                (
                    f"Erreur inconnue Bybit GET {url} | category={category} batch_idx={batch_idx} "
                    f"batch_size={len(symbol_batch)} sample=[{sample}] timeout={timeout}s error={e}"
                )
            )
    
    return batch_spread_data


def _fetch_single_spread(base_url: str, symbol: str, timeout: int, category: str) -> float | None:
    """
    R√©cup√®re le spread pour un seul symbole.
    
    Args:
        base_url (str): URL de base de l'API Bybit
        symbol (str): Symbole √† analyser
        timeout (int): Timeout pour les requ√™tes HTTP
        category (str): Cat√©gorie des symboles ("linear" ou "inverse")
        
    Returns:
        float | None: Spread en pourcentage ou None si erreur
    """
    try:
        url = f"{base_url}/v5/market/tickers"
        params = {
            "category": category,
            "symbol": symbol
        }
        
        rate_limiter = get_rate_limiter()
        rate_limiter.acquire()
        with httpx.Client(timeout=timeout) as client:
            response = client.get(url, params=params)
            
            if response.status_code >= 400:
                raise RuntimeError(
                    (
                        f"Erreur HTTP Bybit GET {url} | category={category} symbol={symbol} "
                        f"timeout={timeout}s status={response.status_code} detail=\"{response.text[:200]}\""
                    )
                )
            
            data = response.json()
            
            if data.get("retCode") != 0:
                ret_code = data.get("retCode")
                ret_msg = data.get("retMsg", "")
                raise RuntimeError(
                    (
                        f"Erreur API Bybit GET {url} | category={category} symbol={symbol} "
                        f"timeout={timeout}s retCode={ret_code} retMsg=\"{ret_msg}\""
                    )
                )
            
            result = data.get("result", {})
            tickers = result.get("list", [])
            
            if not tickers:
                return None
            
            ticker = tickers[0]
            bid1_price = ticker.get("bid1Price")
            ask1_price = ticker.get("ask1Price")
            
            if bid1_price is not None and ask1_price is not None:
                try:
                    bid1 = float(bid1_price)
                    ask1 = float(ask1_price)
                    
                    if bid1 > 0 and ask1 > 0:
                        spread_pct = (ask1 - bid1) / ((ask1 + bid1) / 2)
                        return spread_pct
                except (ValueError, TypeError, ZeroDivisionError):
                    pass
        
        return None
        
    except Exception:
        return None


def filter_by_spread(symbols_data: list[tuple[str, float, float, str]], spread_data: dict[str, float], spread_max: float | None) -> list[tuple[str, float, float, str, float]]:
    """
    Filtre les symboles par spread maximum.
    
    Args:
        symbols_data (list[tuple[str, float, float, str]]): Liste des (symbol, funding, volume, funding_time_remaining)
        spread_data (dict[str, float]): Dictionnaire des spreads {symbol: spread_pct}
        spread_max (float | None): Spread maximum autoris√©
        
    Returns:
        list[tuple[str, float, float, str, float]]: Liste des (symbol, funding, volume, funding_time_remaining, spread_pct) filtr√©s
    """
    if spread_max is None:
        # Pas de filtre de spread, ajouter 0.0 comme spread par d√©faut
        return [(symbol, funding, volume, funding_time_remaining, 0.0) for symbol, funding, volume, funding_time_remaining in symbols_data]
    
    filtered_symbols = []
    for symbol, funding, volume, funding_time_remaining in symbols_data:
        if symbol in spread_data:
            spread_pct = spread_data[symbol]
            if spread_pct <= spread_max:
                filtered_symbols.append((symbol, funding, volume, funding_time_remaining, spread_pct))
    
    return filtered_symbols


async def filter_by_volatility_async(symbols_data: list[tuple[str, float, float, str, float]], bybit_client, volatility_min: float, volatility_max: float, logger, volatility_cache: dict, ttl_seconds: int | None = None, symbol_categories: dict[str, str] | None = None) -> list[tuple[str, float, float, str, float]]:
    """
    Calcule la volatilit√© 5 minutes pour tous les symboles et applique les filtres si d√©finis.
    OPTIMIS√â: Utilise compute_volatility_batch_async() pour parall√©liser les appels API.
    
    Args:
        symbols_data (list[tuple[str, float, float, str, float]]): Liste des (symbol, funding, volume, funding_time_remaining, spread_pct)
        bybit_client: Instance du client Bybit
        volatility_min (float): Volatilit√© minimum autoris√©e (0.002 = 0.2%) ou None
        volatility_max (float): Volatilit√© maximum autoris√©e (0.007 = 0.7%) ou None
        logger: Logger pour les messages
        volatility_cache (dict): Cache de volatilit√© {symbol: (timestamp, volatility)}
        
    Returns:
        list[tuple[str, float, float, str, float]]: Liste avec volatilit√© calcul√©e et filtr√©e si n√©cessaire
    """
    
    filtered_symbols = []
    rejected_count = 0
    
    # S√©parer les symboles en cache et ceux √† calculer
    symbols_to_calculate = []
    cached_volatilities = {}
    
    cache_ttl = ttl_seconds or 120
    for symbol, funding, volume, funding_time_remaining, spread_pct in symbols_data:
        cache_key = get_volatility_cache_key(symbol)
        cached_data = volatility_cache.get(cache_key)
        
        if cached_data and is_cache_valid(cached_data[0], ttl_seconds=cache_ttl):
            # Utiliser la valeur en cache
            cached_volatilities[symbol] = cached_data[1]
        else:
            # Ajouter √† la liste des symboles √† calculer
            symbols_to_calculate.append(symbol)
    
    # OPTIMISATION: Calculer la volatilit√© pour tous les symboles en parall√®le
    if symbols_to_calculate:
        logger.info(f"üîé Calcul volatilit√© async (parall√®le) pour {len(symbols_to_calculate)} symboles‚Ä¶")
        batch_volatilities = await compute_volatility_batch_async(bybit_client, symbols_to_calculate, timeout=10, symbol_categories=symbol_categories)
        
        # Mettre √† jour le cache avec les nouveaux r√©sultats
        for symbol, vol_pct in batch_volatilities.items():
            if vol_pct is not None:
                cache_key = get_volatility_cache_key(symbol)
                volatility_cache[cache_key] = (time.time(), vol_pct)
            cached_volatilities[symbol] = vol_pct
    
    # Appliquer les filtres avec toutes les volatilit√©s (cache + calcul√©es)
    for symbol, funding, volume, funding_time_remaining, spread_pct in symbols_data:
        vol_pct = cached_volatilities.get(symbol)
        
        # Appliquer le filtre de volatilit√© seulement si des seuils sont d√©finis
        if volatility_min is not None or volatility_max is not None:
            if vol_pct is not None:
                # V√©rifier les seuils de volatilit√©
                rejected_reason = None
                if volatility_min is not None and vol_pct < volatility_min:
                    rejected_reason = f"< seuil min {volatility_min:.2%}"
                elif volatility_max is not None and vol_pct > volatility_max:
                    rejected_reason = f"> seuil max {volatility_max:.2%}"
                
                if rejected_reason:
                    rejected_count += 1
                    continue
        
        # Ajouter le symbole avec sa volatilit√©
        filtered_symbols.append((symbol, funding, volume, funding_time_remaining, spread_pct, vol_pct))
    
    # Log des r√©sultats
    kept_count = len(filtered_symbols)
    threshold_info = []
    if volatility_min is not None:
        threshold_info.append(f"min={volatility_min:.2%}")
    if volatility_max is not None:
        threshold_info.append(f"max={volatility_max:.2%}")
    threshold_str = " | ".join(threshold_info) if threshold_info else "aucun"
    logger.info(f"‚úÖ Calcul volatilit√© async: gard√©s={kept_count} | rejet√©s={rejected_count} (seuils: {threshold_str})")
    
    return filtered_symbols


def filter_by_volatility(symbols_data: list[tuple[str, float, float, str, float]], bybit_client, volatility_min: float, volatility_max: float, logger, volatility_cache: dict, ttl_seconds: int | None = None, symbol_categories: dict[str, str] | None = None) -> list[tuple[str, float, float, str, float]]:
    """
    Version synchrone de filter_by_volatility pour compatibilit√©.
    Utilise asyncio.run() pour ex√©cuter la version async.
    """
    return asyncio.run(filter_by_volatility_async(symbols_data, bybit_client, volatility_min, volatility_max, logger, volatility_cache, ttl_seconds, symbol_categories))


def filter_by_funding(perp_data: dict, funding_map: dict, funding_min: float | None, funding_max: float | None, volume_min: float | None, volume_min_millions: float | None, limite: int | None, funding_time_min_minutes: int | None = None, funding_time_max_minutes: int | None = None) -> list[tuple[str, float, float, str]]:
    """
    Filtre les symboles par funding, volume et fen√™tre temporelle avant funding, puis trie par |funding| d√©croissant.
    
    Args:
        perp_data (dict): Donn√©es des perp√©tuels (linear, inverse, total)
        funding_map (dict): Dictionnaire des funding rates, volumes et temps de funding
        funding_min (float | None): Funding minimum
        funding_max (float | None): Funding maximum
        volume_min (float | None): Volume minimum (ancien format)
        volume_min_millions (float | None): Volume minimum en millions (nouveau format)
        limite (int | None): Limite du nombre d'√©l√©ments
        funding_time_min_minutes (int | None): Temps minimum en minutes avant prochain funding
        funding_time_max_minutes (int | None): Temps maximum en minutes avant prochain funding
        
    Returns:
        list[tuple[str, float, float, str]]: Liste des (symbol, funding, volume, funding_time_remaining) tri√©s
    """
    # R√©cup√©rer tous les symboles perp√©tuels
    all_symbols = list(set(perp_data["linear"] + perp_data["inverse"]))
    
    # D√©terminer le volume minimum √† utiliser (priorit√© : volume_min_millions > volume_min)
    effective_volume_min = None
    if volume_min_millions is not None:
        effective_volume_min = volume_min_millions * 1_000_000  # Convertir en valeur brute
    elif volume_min is not None:
        effective_volume_min = volume_min
    
    # Filtrer par funding, volume et fen√™tre temporelle
    filtered_symbols = []
    for symbol in all_symbols:
        if symbol in funding_map:
            data = funding_map[symbol]
            funding = data["funding"]
            volume = data["volume"]
            next_funding_time = data.get("next_funding_time")
            
            # Appliquer les bornes funding/volume
            if funding_min is not None and funding < funding_min:
                continue
            if funding_max is not None and funding > funding_max:
                continue
            if effective_volume_min is not None and volume < effective_volume_min:
                continue
            
            # Appliquer le filtre temporel si demand√©
            if funding_time_min_minutes is not None or funding_time_max_minutes is not None:
                # Convertir next_funding_time en minutes restantes
                minutes_remaining: float | None = None
                try:
                    import datetime
                    if next_funding_time:
                        if isinstance(next_funding_time, (int, float)):
                            target_dt = datetime.datetime.fromtimestamp(float(next_funding_time) / 1000, tz=datetime.timezone.utc)
                        elif isinstance(next_funding_time, str):
                            if next_funding_time.isdigit():
                                target_dt = datetime.datetime.fromtimestamp(float(next_funding_time) / 1000, tz=datetime.timezone.utc)
                            else:
                                target_dt = datetime.datetime.fromisoformat(next_funding_time.replace('Z', '+00:00'))
                                if target_dt.tzinfo is None:
                                    target_dt = target_dt.replace(tzinfo=datetime.timezone.utc)
                        else:
                            target_dt = None
                        if target_dt is not None:
                            now_dt = datetime.datetime.now(datetime.timezone.utc)
                            delta_sec = (target_dt - now_dt).total_seconds()
                            if delta_sec > 0:
                                minutes_remaining = delta_sec / 60.0
                except Exception:
                    minutes_remaining = None
                
                # Si pas de temps valide alors qu'on filtre, rejeter
                if minutes_remaining is None:
                    continue
                if funding_time_min_minutes is not None and minutes_remaining < float(funding_time_min_minutes):
                    continue
                if funding_time_max_minutes is not None and minutes_remaining > float(funding_time_max_minutes):
                    continue
            
            # Calculer le temps restant avant le prochain funding (format√©)
            funding_time_remaining = calculate_funding_time_remaining(next_funding_time)
            
            filtered_symbols.append((symbol, funding, volume, funding_time_remaining))
    
    # Trier par |funding| d√©croissant
    filtered_symbols.sort(key=lambda x: abs(x[1]), reverse=True)
    
    # Appliquer la limite
    if limite is not None:
        filtered_symbols = filtered_symbols[:limite]
    
    return filtered_symbols


class PriceTracker:
    """Suivi des prix en temps r√©el via WebSocket avec filtrage par funding."""
    
    def __init__(self):
        self.logger = setup_logging()
        self.running = True
        self.ws = None
        self.display_thread = None
        self.symbols = []
        self.funding_data = {}
        self.original_funding_data = {}  # Donn√©es de funding originales avec next_funding_time
        self.volatility_cache = {}  # Cache pour la volatilit√© {symbol: (timestamp, volatility)}
        # self.start_time supprim√©: on s'appuie uniquement sur nextFundingTime c√¥t√© Bybit
        self.realtime_data = {}  # Donn√©es en temps r√©el via WebSocket {symbol: {funding_rate, volume24h, bid1, ask1, next_funding_time, ...}}
        self._realtime_lock = threading.Lock()  # Verrou pour prot√©ger realtime_data
        self._ws_conns: list[PublicWSConnection] = []
        self._ws_threads: list[threading.Thread] = []
        self._vol_refresh_thread: threading.Thread | None = None
        self.symbol_categories: dict[str, str] = {}
        
        # Configuration
        settings = get_settings()
        self.testnet = settings['testnet']
        
        # Configuration du signal handler pour Ctrl+C
        signal.signal(signal.SIGINT, self._signal_handler)
        
        self.logger.info("üöÄ Orchestrateur du bot (filters + WebSocket prix)")
        self.logger.info("üìÇ Configuration charg√©e")
        
        # D√©marrer le monitoring des m√©triques
        start_metrics_monitoring(interval_minutes=5)
    
    def _signal_handler(self, signum, frame):
        """Gestionnaire de signal pour Ctrl+C."""
        self.logger.info("üßπ Arr√™t demand√©, fermeture de la WebSocket‚Ä¶")
        self.running = False
        # Fermer toutes les connexions WS isol√©es
        try:
            for conn in getattr(self, "_ws_conns", []) or []:
                conn.close()
        except Exception:
            pass
        return
    
    def _update_realtime_data(self, symbol: str, ticker_data: dict):
        """
        Met √† jour les donn√©es en temps r√©el pour un symbole donn√©.
        
        Args:
            symbol (str): Symbole √† mettre √† jour
            ticker_data (dict): Donn√©es du ticker re√ßues via WebSocket
        """
        try:
            # Construire un diff et fusionner avec l'√©tat pr√©c√©dent pour ne pas √©craser des valeurs valides par None
            now_ts = time.time()
            incoming = {
                'funding_rate': ticker_data.get('fundingRate'),
                'volume24h': ticker_data.get('volume24h'),
                'bid1_price': ticker_data.get('bid1Price'),
                'ask1_price': ticker_data.get('ask1Price'),
                'next_funding_time': ticker_data.get('nextFundingTime'),
                'mark_price': ticker_data.get('markPrice'),
                'last_price': ticker_data.get('lastPrice'),
            }
            if any(incoming[key] is not None for key in ['funding_rate', 'volume24h', 'bid1_price', 'ask1_price', 'next_funding_time']):
                with self._realtime_lock:
                    current = self.realtime_data.get(symbol, {})
                    merged = dict(current) if current else {}
                    for k, v in incoming.items():
                        if v is not None:
                            merged[k] = v
                    merged['timestamp'] = now_ts
                    self.realtime_data[symbol] = merged
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erreur mise √† jour donn√©es temps r√©el pour {symbol}: {e}")
    
    
    def _recalculate_funding_time(self, symbol: str) -> str:
        """Retourne le temps restant bas√© uniquement sur nextFundingTime (WS puis REST)."""
        try:
            with self._realtime_lock:
                realtime_info = self.realtime_data.get(symbol, {})
            ws_ts = realtime_info.get('next_funding_time')
            if ws_ts:
                return calculate_funding_time_remaining(ws_ts)
            rest_ts = self.original_funding_data.get(symbol)
            if rest_ts:
                return calculate_funding_time_remaining(rest_ts)
            return "-"
        except Exception:
            return "-"
    
    def _print_price_table(self):
        """Affiche le tableau des prix align√© avec funding, volume en millions, spread et volatilit√©."""
        # Purger les donn√©es de prix trop anciennes et r√©cup√©rer un snapshot
        try:
            purge_expired(ttl_seconds=getattr(self, "price_ttl_sec", 120))
        except Exception:
            pass
        snapshot = get_snapshot()
        
        if not snapshot:
            print("Aucune donn√©e de prix disponible")
            return
        
        # Calculer les largeurs de colonnes
        all_symbols = list(self.funding_data.keys())
        max_symbol_len = max(len("Symbole"), max(len(s) for s in all_symbols)) if all_symbols else len("Symbole")
        symbol_w = max(8, max_symbol_len)
        funding_w = 12  # Largeur pour le funding
        volume_w = 10  # Largeur pour le volume en millions
        spread_w = 10  # Largeur pour le spread
        volatility_w = 12  # Largeur pour la volatilit√©
        funding_time_w = 15  # Largeur pour le temps de funding (avec secondes)
        
        # En-t√™te
        header = f"{'Symbole':<{symbol_w}} | {'Funding %':>{funding_w}} | {'Volume (M)':>{volume_w}} | {'Spread %':>{spread_w}} | {'Volatilit√© %':>{volatility_w}} | {'Funding T':>{funding_time_w}}"
        sep = f"{'-'*symbol_w}-+-{'-'*funding_w}-+-{'-'*volume_w}-+-{'-'*spread_w}-+-{'-'*volatility_w}-+-{'-'*funding_time_w}"
        
        print("\n" + header)
        print(sep)
        
        # Donn√©es
        for symbol, data in self.funding_data.items():
            # R√©cup√©rer les donn√©es en temps r√©el si disponibles
            realtime_info = self.realtime_data.get(symbol, {})
            # R√©cup√©rer les valeurs initiales (REST) comme fallbacks
            # data format: (funding, volume, funding_time_remaining, spread_pct, volatility_pct)
            try:
                original_funding = data[0]
                original_volume = data[1]
                original_funding_time = data[2]
            except Exception:
                original_funding = None
                original_volume = None
                original_funding_time = "-"
            
            # Utiliser les donn√©es en temps r√©el si disponibles, sinon fallback vers REST initial
            if realtime_info:
                # Donn√©es en temps r√©el disponibles - g√©rer les valeurs None
                funding_rate = realtime_info.get('funding_rate')
                funding = float(funding_rate) if funding_rate is not None else original_funding
                
                volume24h = realtime_info.get('volume24h')
                volume = float(volume24h) if volume24h is not None else original_volume
                
                # On ne remplace pas la valeur initiale si la donn√©e WS est absente
                funding_time_remaining = original_funding_time
                
                # Calculer le spread en temps r√©el si on a bid/ask
                spread_pct = None
                if realtime_info.get('bid1_price') and realtime_info.get('ask1_price'):
                    try:
                        bid_price = float(realtime_info['bid1_price'])
                        ask_price = float(realtime_info['ask1_price'])
                        if bid_price > 0 and ask_price > 0:
                            mid_price = (ask_price + bid_price) / 2
                            if mid_price > 0:
                                spread_pct = (ask_price - bid_price) / mid_price
                    except (ValueError, TypeError):
                        pass  # Garder spread_pct = None en cas d'erreur
                # Fallback: utiliser la valeur REST calcul√©e au filtrage si le temps r√©el est indisponible
                if spread_pct is None:
                    try:
                        # data[3] contient spread_pct (ou 0.0 si absent lors du filtrage)
                        spread_pct = data[3]
                    except Exception:
                        pass
                
                # Volatilit√©: lecture cache uniquement (pas d'appel r√©seau dans l'affichage)
                volatility_pct = None
                cache_key = get_volatility_cache_key(symbol)
                cached_data = self.volatility_cache.get(cache_key)
                # N'afficher que si le cache est encore valide (pas de donn√©es p√©rim√©es)
                if cached_data and is_cache_valid(cached_data[0], ttl_seconds=getattr(self, "volatility_ttl_sec", 120)):
                    volatility_pct = cached_data[1]
            else:
                # Pas de donn√©es en temps r√©el disponibles - utiliser les valeurs initiales REST
                funding = original_funding
                volume = original_volume
                funding_time_remaining = original_funding_time
                # Fallback: utiliser la valeur REST calcul√©e au filtrage
                spread_pct = None
                volatility_pct = None
                
                # Volatilit√©: lecture cache uniquement (pas d'appel r√©seau dans l'affichage)
                if volatility_pct is None:
                    cache_key = get_volatility_cache_key(symbol)
                    cached_data = self.volatility_cache.get(cache_key)
                    if cached_data and is_cache_valid(cached_data[0], ttl_seconds=getattr(self, "volatility_ttl_sec", 120)):
                        volatility_pct = cached_data[1]
                # Essayer de r√©cup√©rer le spread initial calcul√© lors du filtrage
                if spread_pct is None:
                    try:
                        spread_pct = data[3]
                    except Exception:
                        pass
            
            # Recalculer le temps de funding (priorit√© WS, fallback REST)
            current_funding_time = self._recalculate_funding_time(symbol)
            if current_funding_time is None:
                current_funding_time = "-"
            
            # G√©rer l'affichage des valeurs null
            if funding is not None:
                funding_pct = funding * 100.0
                funding_str = f"{funding_pct:+{funding_w-1}.4f}%"
            else:
                funding_str = "null"
            
            if volume is not None and volume > 0:
                volume_millions = volume / 1_000_000
                volume_str = f"{volume_millions:,.1f}"
            else:
                volume_str = "null"
            
            if spread_pct is not None:
                spread_pct_display = spread_pct * 100.0
                spread_str = f"{spread_pct_display:+.3f}%"
            else:
                spread_str = "null"
            
            # Formatage de la volatilit√©
            if volatility_pct is not None:
                volatility_pct_display = volatility_pct * 100.0
                volatility_str = f"{volatility_pct_display:+.3f}%"
            else:
                volatility_str = "-"
            
            line = f"{symbol:<{symbol_w}} | {funding_str:>{funding_w}} | {volume_str:>{volume_w}} | {spread_str:>{spread_w}} | {volatility_str:>{volatility_w}} | {current_funding_time:>{funding_time_w}}"
            print(line)
        
        print()  # Ligne vide apr√®s le tableau
    
    def _display_loop(self):
        """Boucle d'affichage toutes les 15 secondes."""
        while self.running:
            self._print_price_table()
            
            # Attendre 15 secondes
            for _ in range(150):  # 150 * 0.1s = 15s
                if not self.running:
                    break
                time.sleep(0.1)
    
    # Anciennes callbacks WS supprim√©es (remplac√©es par PublicWSConnection)
    
    def start(self):
        """D√©marre le suivi des prix avec filtrage par funding."""
        # Charger et valider la configuration
        try:
            config = load_config()
        except ValueError as e:
            self.logger.error(f"‚ùå Erreur de configuration : {e}")
            self.logger.error("üí° Corrigez les param√®tres dans src/parameters.yaml ou les variables d'environnement")
            return  # Arr√™t propre sans sys.exit
        
        # V√©rifier si le fichier de config existe
        config_path = "src/parameters.yaml"
        if not os.path.exists(config_path):
            self.logger.info("‚ÑπÔ∏è Aucun fichier de param√®tres trouv√© (src/parameters.yaml) ‚Üí utilisation des valeurs par d√©faut.")
        else:
            self.logger.info("‚ÑπÔ∏è Configuration charg√©e depuis src/parameters.yaml")
        
        # Cr√©er un client PUBLIC pour r√©cup√©rer l'URL publique (aucune cl√© requise)
        client = BybitPublicClient(
            testnet=self.testnet,
            timeout=10,
        )
        
        base_url = client.public_base_url()
        
        # R√©cup√©rer l'univers perp
        perp_data = get_perp_symbols(base_url, timeout=10)
        self.logger.info(f"üó∫Ô∏è Univers perp r√©cup√©r√© : linear={len(perp_data['linear'])} | inverse={len(perp_data['inverse'])} | total={perp_data['total']}")
        # Stocker le mapping officiel des cat√©gories
        try:
            self.symbol_categories = perp_data.get("categories", {}) or {}
        except Exception:
            self.symbol_categories = {}
        
        # Extraire les param√®tres de configuration
        categorie = config.get("categorie", "both")
        funding_min = config.get("funding_min")
        funding_max = config.get("funding_max")
        volume_min = config.get("volume_min")
        volume_min_millions = config.get("volume_min_millions")
        spread_max = config.get("spread_max")
        volatility_min = config.get("volatility_min")
        volatility_max = config.get("volatility_max")
        limite = config.get("limite")
        self.volatility_ttl_sec = int(config.get("volatility_ttl_sec", 120) or 120)
        self.price_ttl_sec = 120
        # Nouveaux param√®tres temporels
        funding_time_min_minutes = config.get("funding_time_min_minutes")
        funding_time_max_minutes = config.get("funding_time_max_minutes")
        
        # Afficher les filtres
        min_display = f"{funding_min:.6f}" if funding_min is not None else "none"
        max_display = f"{funding_max:.6f}" if funding_max is not None else "none"
        volume_display = f"{volume_min_millions:.1f}" if volume_min_millions is not None else "none"
        spread_display = f"{spread_max:.4f}" if spread_max is not None else "none"
        volatility_min_display = f"{volatility_min:.3f}" if volatility_min is not None else "none"
        volatility_max_display = f"{volatility_max:.3f}" if volatility_max is not None else "none"
        limite_display = str(limite) if limite is not None else "none"
        ft_min_display = str(funding_time_min_minutes) if funding_time_min_minutes is not None else "none"
        ft_max_display = str(funding_time_max_minutes) if funding_time_max_minutes is not None else "none"
        
        self.logger.info(f"üéõÔ∏è Filtres | cat√©gorie={categorie} | funding_min={min_display} | funding_max={max_display} | volume_min_millions={volume_display} | spread_max={spread_display} | volatility_min={volatility_min_display} | volatility_max={volatility_max_display} | ft_min(min)={ft_min_display} | ft_max(min)={ft_max_display} | limite={limite_display} | vol_ttl={self.volatility_ttl_sec}s")
        
        # R√©cup√©rer les funding rates selon la cat√©gorie
        # OPTIMISATION: fetch_funding_map() utilise maintenant la limite maximum (1000) de l'API
        funding_map = {}
        if categorie == "linear":
            self.logger.info("üì° R√©cup√©ration des funding rates pour linear (optimis√©)‚Ä¶")
            funding_map = fetch_funding_map(base_url, "linear", 10)
        elif categorie == "inverse":
            self.logger.info("üì° R√©cup√©ration des funding rates pour inverse (optimis√©)‚Ä¶")
            funding_map = fetch_funding_map(base_url, "inverse", 10)
        else:  # "both"
            self.logger.info("üì° R√©cup√©ration des funding rates pour linear+inverse (optimis√©)‚Ä¶")
            linear_funding = fetch_funding_map(base_url, "linear", 10)
            inverse_funding = fetch_funding_map(base_url, "inverse", 10)
            funding_map = {**linear_funding, **inverse_funding}  # Merger (priorit√© au dernier)
        
        if not funding_map:
            self.logger.warning("‚ö†Ô∏è Aucun funding disponible pour la cat√©gorie s√©lectionn√©e")
            raise FundingUnavailableError("Aucun funding disponible pour la cat√©gorie s√©lectionn√©e")
        
        # Stocker les next_funding_time originaux pour fallback (REST)
        try:
            self.original_funding_data = {}
            for _sym, _data in funding_map.items():
                try:
                    nft = _data.get("next_funding_time")
                    if nft:
                        self.original_funding_data[_sym] = nft
                except Exception:
                    continue
        except Exception:
            # En cas d'erreur, on garde simplement la map vide
            self.original_funding_data = {}
        
        # Compter les symboles avant filtrage
        all_symbols = list(set(perp_data["linear"] + perp_data["inverse"]))
        n0 = len([s for s in all_symbols if s in funding_map])
        
        # Filtrer par funding, volume et temps avant funding
        filtered_symbols = filter_by_funding(
            perp_data,
            funding_map,
            funding_min,
            funding_max,
            volume_min,
            volume_min_millions,
            limite,
            funding_time_min_minutes=funding_time_min_minutes,
            funding_time_max_minutes=funding_time_max_minutes,
        )
        n1 = len(filtered_symbols)
        
        # Appliquer le filtre de spread si n√©cessaire
        final_symbols = filtered_symbols
        n2 = n1
        
        if spread_max is not None and filtered_symbols:
            # R√©cup√©rer les donn√©es de spread pour les symboles restants
            symbols_to_check = [symbol for symbol, _, _, _ in filtered_symbols]
            self.logger.info(f"üîé √âvaluation du spread (REST tickers) pour {len(symbols_to_check)} symboles‚Ä¶")
            
            try:
                spread_data = {}
                
                # S√©parer les symboles par cat√©gorie pour les requ√™tes spread
                # Utiliser les cat√©gories officielles (fallback heuristique si absent)
                linear_symbols_for_spread = [s for s in symbols_to_check if category_of_symbol(s, self.symbol_categories) == "linear"]
                inverse_symbols_for_spread = [s for s in symbols_to_check if category_of_symbol(s, self.symbol_categories) == "inverse"]
                
                # OPTIMISATION: fetch_spread_data() utilise maintenant des batches de 200 et de la parall√©lisation
                # R√©cup√©rer les spreads pour chaque cat√©gorie
                if linear_symbols_for_spread:
                    self.logger.info(f"üîé R√©cup√©ration spreads linear (optimis√©: batch=200, parall√®le) pour {len(linear_symbols_for_spread)} symboles‚Ä¶")
                    linear_spread_data = fetch_spread_data(base_url, linear_symbols_for_spread, 10, "linear")
                    spread_data.update(linear_spread_data)
                
                if inverse_symbols_for_spread:
                    self.logger.info(f"üîé R√©cup√©ration spreads inverse (optimis√©: batch=200, parall√®le) pour {len(inverse_symbols_for_spread)} symboles‚Ä¶")
                    inverse_spread_data = fetch_spread_data(base_url, inverse_symbols_for_spread, 10, "inverse")
                    spread_data.update(inverse_spread_data)
                
                final_symbols = filter_by_spread(filtered_symbols, spread_data, spread_max)
                n2 = len(final_symbols)
                
                # Log des r√©sultats du filtre spread
                rejected = n1 - n2
                spread_pct_display = spread_max * 100
                self.logger.info(f"‚úÖ Filtre spread : gard√©s={n2} | rejet√©s={rejected} (seuil {spread_pct_display:.2f}%)")
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erreur lors de la r√©cup√©ration des spreads : {e}")
                # Continuer sans le filtre de spread
                final_symbols = [(symbol, funding, volume, funding_time_remaining, 0.0) for symbol, funding, volume, funding_time_remaining in filtered_symbols]
        
        # Calculer la volatilit√© pour tous les symboles (m√™me sans filtre)
        if final_symbols:
            try:
                self.logger.info("üîé √âvaluation de la volatilit√© 5m pour tous les symboles‚Ä¶")
                final_symbols = filter_by_volatility(
                    final_symbols,
                    client,
                    volatility_min,
                    volatility_max,
                    self.logger,
                    self.volatility_cache,
                    ttl_seconds=self.volatility_ttl_sec,
                    symbol_categories=self.symbol_categories,
                )
                n2 = len(final_symbols)
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erreur lors du calcul de la volatilit√© : {e}")
                # Continuer sans le filtre de volatilit√©
        
        # Appliquer la limite finale
        if limite is not None and len(final_symbols) > limite:
            final_symbols = final_symbols[:limite]
        n3 = len(final_symbols)
        
        # Enregistrer les m√©triques des filtres
        record_filter_result("funding_volume_time", n1, n0 - n1)
        if spread_max is not None:
            record_filter_result("spread", n2, n1 - n2)
        record_filter_result("volatility", n2, n2 - n2)  # Pas de rejet par volatilit√© dans ce cas
        record_filter_result("final_limit", n3, n2 - n3)
        
        # Log des comptes
        self.logger.info(f"üßÆ Comptes | avant filtres = {n0} | apr√®s funding/volume/temps = {n1} | apr√®s spread = {n2} | apr√®s volatilit√© = {n2} | apr√®s tri+limit = {n3}")
        
        if not final_symbols:
            self.logger.warning("‚ö†Ô∏è Aucun symbole ne correspond aux crit√®res de filtrage")
            raise NoSymbolsError("Aucun symbole ne correspond aux crit√®res de filtrage")
        
        # Log des symboles retenus
        if final_symbols:
            symbols_list = [symbol for symbol, _, _, _, _, _ in final_symbols] if len(final_symbols[0]) == 6 else [symbol for symbol, _, _, _, _ in final_symbols] if len(final_symbols[0]) == 5 else [symbol for symbol, _, _, _ in final_symbols] if len(final_symbols[0]) == 4 else [symbol for symbol, _, _ in final_symbols]
            self.logger.info(f"üß≠ Symboles retenus (Top {n3}) : {symbols_list}")
            
            # S√©parer les symboles par cat√©gorie
            if len(final_symbols[0]) == 6:
                linear_symbols = [symbol for symbol, _, _, _, _, _ in final_symbols if category_of_symbol(symbol, self.symbol_categories) == "linear"]
                inverse_symbols = [symbol for symbol, _, _, _, _, _ in final_symbols if category_of_symbol(symbol, self.symbol_categories) == "inverse"]
            elif len(final_symbols[0]) == 5:
                linear_symbols = [symbol for symbol, _, _, _, _ in final_symbols if category_of_symbol(symbol, self.symbol_categories) == "linear"]
                inverse_symbols = [symbol for symbol, _, _, _, _ in final_symbols if category_of_symbol(symbol, self.symbol_categories) == "inverse"]
            elif len(final_symbols[0]) == 4:
                linear_symbols = [symbol for symbol, _, _, _ in final_symbols if category_of_symbol(symbol, self.symbol_categories) == "linear"]
                inverse_symbols = [symbol for symbol, _, _, _ in final_symbols if category_of_symbol(symbol, self.symbol_categories) == "inverse"]
            else:
                linear_symbols = [symbol for symbol, _, _ in final_symbols if category_of_symbol(symbol, self.symbol_categories) == "linear"]
                inverse_symbols = [symbol for symbol, _, _ in final_symbols if category_of_symbol(symbol, self.symbol_categories) == "inverse"]
            
            self.linear_symbols = linear_symbols
            self.inverse_symbols = inverse_symbols
            
            # Construire funding_data avec les bonnes donn√©es
            if len(final_symbols[0]) == 6:
                # Format: (symbol, funding, volume, funding_time_remaining, spread_pct, volatility_pct)
                self.funding_data = {symbol: (funding, volume, funding_time_remaining, spread_pct, volatility_pct) for symbol, funding, volume, funding_time_remaining, spread_pct, volatility_pct in final_symbols}
            elif len(final_symbols[0]) == 5:
                # Format: (symbol, funding, volume, funding_time_remaining, spread_pct)
                self.funding_data = {symbol: (funding, volume, funding_time_remaining, spread_pct, None) for symbol, funding, volume, funding_time_remaining, spread_pct in final_symbols}
            elif len(final_symbols[0]) == 4:
                # Format: (symbol, funding, volume, funding_time_remaining)
                self.funding_data = {symbol: (funding, volume, funding_time_remaining, 0.0, None) for symbol, funding, volume, funding_time_remaining in final_symbols}
            else:
                # Format: (symbol, funding, volume)
                self.funding_data = {symbol: (funding, volume, "-", 0.0, None) for symbol, funding, volume in final_symbols}
            
            # Les donn√©es en temps r√©el seront initialis√©es uniquement via WebSocket
            # Pas d'initialisation avec des donn√©es statiques pour garantir la fra√Æcheur
        
        self.logger.info(f"üìä Symboles linear: {len(self.linear_symbols)}, inverse: {len(self.inverse_symbols)}")
        
        # D√©marrer la t√¢che de rafra√Æchissement de la volatilit√© (arri√®re-plan) AVANT les WS bloquantes
        self._start_volatility_refresh_task()

        # D√©marrer les connexions WebSocket selon les symboles disponibles
        if self.linear_symbols and self.inverse_symbols:
            # Les deux cat√©gories : cr√©er deux connexions
            self.logger.info("üîÑ D√©marrage des connexions WebSocket pour linear et inverse")
            self._start_dual_connections()
        elif self.linear_symbols:
            # Seulement linear
            self.logger.info("üîÑ D√©marrage de la connexion WebSocket linear")
            self._start_single_connection("linear", self.linear_symbols)
        elif self.inverse_symbols:
            # Seulement inverse
            self.logger.info("üîÑ D√©marrage de la connexion WebSocket inverse")
            self._start_single_connection("inverse", self.inverse_symbols)
        else:
            self.logger.warning("‚ö†Ô∏è Aucun symbole valide trouv√©")
            raise NoSymbolsError("Aucun symbole valide trouv√©")
    
    def _handle_ticker(self, ticker_data: dict):
        """Callback thread-safe appel√© par les connexions WS isol√©es pour chaque tick."""
        try:
            symbol = ticker_data.get("symbol", "")
            mark_price = ticker_data.get("markPrice")
            last_price = ticker_data.get("lastPrice")
            if symbol and mark_price is not None and last_price is not None:
                mark_val = float(mark_price)
                last_val = float(last_price)
                update(symbol, mark_val, last_val, time.time())
            # Mettre √† jour realtime_data (autres champs aussi utiles)
            if symbol:
                self._update_realtime_data(symbol, ticker_data)
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erreur handle_ticker: {e}")

    def _start_single_connection(self, category: str, symbols: list):
        """D√©marre une connexion WebSocket pour une seule cat√©gorie via une instance isol√©e."""
        conn = PublicWSConnection(category=category, symbols=symbols, testnet=self.testnet, logger=self.logger, on_ticker_callback=self._handle_ticker)
        self._ws_conns = [conn]
        # D√©marrer l'affichage
        self.display_thread = threading.Thread(target=self._display_loop)
        self.display_thread.daemon = True
        self.display_thread.start()
        # Lancer la connexion (bloquant)
        conn.run()
    
    def _start_dual_connections(self):
        """D√©marre deux connexions WebSocket isol√©es (linear et inverse)."""
        # D√©marrer l'affichage
        self.display_thread = threading.Thread(target=self._display_loop)
        self.display_thread.daemon = True
        self.display_thread.start()
        # Cr√©er connexions isol√©es
        linear_conn = PublicWSConnection(category="linear", symbols=self.linear_symbols, testnet=self.testnet, logger=self.logger, on_ticker_callback=self._handle_ticker)
        inverse_conn = PublicWSConnection(category="inverse", symbols=self.inverse_symbols, testnet=self.testnet, logger=self.logger, on_ticker_callback=self._handle_ticker)
        self._ws_conns = [linear_conn, inverse_conn]
        # Lancer en parall√®le
        linear_thread = threading.Thread(target=linear_conn.run)
        inverse_thread = threading.Thread(target=inverse_conn.run)
        linear_thread.daemon = True
        inverse_thread.daemon = True
        self._ws_threads = [linear_thread, inverse_thread]
        linear_thread.start()
        inverse_thread.start()
        # Bloquer le thread principal sur les deux
        linear_thread.join()
        inverse_thread.join()
        
    def _start_volatility_refresh_task(self):
        """D√©marre une t√¢che en arri√®re-plan pour rafra√Æchir le cache de volatilit√©."""
        if self._vol_refresh_thread and self._vol_refresh_thread.is_alive():
            try:
                self.logger.info("‚ÑπÔ∏è Thread volatilit√© d√©j√† actif")
            except Exception:
                pass
            return
        self._vol_refresh_thread = threading.Thread(target=self._volatility_refresh_loop)
        self._vol_refresh_thread.daemon = True
        self._vol_refresh_thread.start()
        try:
            self.logger.info("üßµ Thread volatilit√© d√©marr√©")
        except Exception:
            pass

    def _volatility_refresh_loop(self):
        """Boucle de rafra√Æchissement p√©riodique (2 min) du cache de volatilit√©."""
        # Utiliser un client PUBLIC (pas besoin de cl√©s) pour la volatilit√©
        try:
            client = BybitPublicClient(
                testnet=self.testnet,
                timeout=10,
            )
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Impossible d'initialiser le client public pour la volatilit√©: {e}")
            return
        # Rafra√Æchir avant l'expiration du TTL pour rester frais; si erreur, ne pas mettre √† jour
        try:
            ttl_sec = int(getattr(self, "volatility_ttl_sec", 120) or 120)
        except Exception:
            ttl_sec = 120
        # Rafra√Æchir plus fr√©quemment que TTL pour r√©duire le risque de trou; plafonner √† 60s
        refresh_interval = max(30, min(60, ttl_sec - 10))
        try:
            self.logger.info(f"ü©∫ Volatilit√©: thread actif | ttl={ttl_sec}s | interval={refresh_interval}s")
        except Exception:
            pass
        while self.running:
            try:
                symbols_to_refresh = list(self.funding_data.keys())
                if not symbols_to_refresh:
                    # Rien √† faire, patienter un court instant
                    time.sleep(5)
                    continue
                # Log de cycle
                try:
                    self.logger.info(f"üîÑ Refresh volatilit√©: {len(symbols_to_refresh)} symboles")
                except Exception:
                    pass
                if symbols_to_refresh:
                    # Utiliser la fonction async batch existante
                    results = asyncio.run(
                        compute_volatility_batch_async(
                            client,
                            symbols_to_refresh,
                            timeout=10,
                            symbol_categories=self.symbol_categories,
                        )
                    )
                    now_ts = time.time()
                    ok_count = 0
                    fail_count = 0
                    for sym, vol_pct in results.items():
                        if vol_pct is not None:
                            cache_key = get_volatility_cache_key(sym)
                            self.volatility_cache[cache_key] = (now_ts, vol_pct)
                            ok_count += 1
                        else:
                            # Ne pas √©craser une valeur fra√Æche par None
                            fail_count += 1
                    try:
                        self.logger.info(f"‚úÖ Refresh volatilit√© termin√©: ok={ok_count} | fail={fail_count}")
                    except Exception:
                        pass
                    # Retry simple pour les symboles en √©chec afin de limiter les fen√™tres "-"
                    failed = [s for s, v in results.items() if v is None]
                    if failed:
                        try:
                            self.logger.info(f"üîÅ Retry volatilit√© pour {len(failed)} symboles‚Ä¶")
                            time.sleep(5)
                            retry_results = asyncio.run(
                                compute_volatility_batch_async(
                                    client,
                                    failed,
                                    timeout=10,
                                    symbol_categories=self.symbol_categories,
                                )
                            )
                            now_ts = time.time()
                            retry_ok = 0
                            for sym, vol_pct in retry_results.items():
                                if vol_pct is not None:
                                    cache_key = get_volatility_cache_key(sym)
                                    self.volatility_cache[cache_key] = (now_ts, vol_pct)
                                    retry_ok += 1
                            self.logger.info(f"üîÅ Retry volatilit√© termin√©: r√©cup√©r√©s={retry_ok}/{len(failed)}")
                        except Exception as re:
                            self.logger.warning(f"‚ö†Ô∏è Erreur retry volatilit√©: {re}")
                # Nettoyer le cache des symboles non suivis
                try:
                    active = set(self.funding_data.keys())
                    stale_keys = [k for k in list(self.volatility_cache.keys()) if k.split("volatility_5m_")[-1] not in active]
                    for k in stale_keys:
                        self.volatility_cache.pop(k, None)
                except Exception:
                    pass
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erreur refresh volatilit√©: {e}")
                # Backoff simple en cas d'erreur globale du cycle
                time.sleep(5)
            # Attendre 2 minutes
            for _ in range(refresh_interval):
                if not self.running:
                    break
                time.sleep(1)
    
    # Runners legacy supprim√©s (isolation par PublicWSConnection en place)


def main():
    """Fonction principale."""
    tracker = PriceTracker()
    try:
        tracker.start()
    except Exception as e:
        tracker.logger.error(f"‚ùå Erreur : {e}")
        tracker.running = False
        # Laisser le code appelant d√©cider (ne pas sys.exit ici)


if __name__ == "__main__":
    main()
